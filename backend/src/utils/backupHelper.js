const { exec } = require('child_process');
const { promisify } = require('util');
const fs = require('fs').promises;
const path = require('path');
const zlib = require('zlib');
const { pipeline } = require('stream/promises');
const mongoose = require('mongoose');

const execAsync = promisify(exec);

/**
 * ‚úÖ T·∫°o backup MongoDB b·∫±ng mongodump
 * @param {string} outputDir - Th∆∞ m·ª•c l∆∞u backup
 * @param {string} dbName - T√™n database (t·ª´ connection string)
 * @returns {Promise<{filePath: string, fileSize: number}>}
 */
async function createMongoBackup(outputDir, dbName) {
  try {
    // ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
    await fs.mkdir(outputDir, { recursive: true });

    // T·∫°o t√™n file backup v·ªõi timestamp
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
    const backupFileName = `backup-${timestamp}.gz`;
    const backupFilePath = path.join(outputDir, backupFileName);

    // L·∫•y MongoDB connection string t·ª´ environment
    const mongoUri = process.env.MONGODB_URI || process.env.MONGO_URI || 'mongodb://localhost:27017/eduschool';
    
    // Parse connection string ƒë·ªÉ l·∫•y database name
    let actualDbName = dbName;
    if (!actualDbName && mongoUri.includes('/')) {
      const uriParts = mongoUri.split('/');
      actualDbName = uriParts[uriParts.length - 1].split('?')[0];
    }
    if (!actualDbName) {
      actualDbName = 'eduschool'; // Fallback
    }

    // T·∫°o temp directory cho mongodump
    const tempDir = path.join(outputDir, `temp-${timestamp}`);
    await fs.mkdir(tempDir, { recursive: true });

    // Ch·∫°y mongodump
    const mongodumpCmd = `mongodump --uri="${mongoUri}" --db="${actualDbName}" --out="${tempDir}"`;
    
    console.log(`üîÑ [Backup] ƒêang ch·∫°y mongodump...`);
    const { stdout, stderr } = await execAsync(mongodumpCmd);
    
    if (stderr && !stderr.includes('writing') && !stderr.includes('done')) {
      console.warn(`‚ö†Ô∏è [Backup] mongodump stderr: ${stderr}`);
    }

    // N√©n backup th√†nh file .gz
    console.log(`üîÑ [Backup] ƒêang n√©n backup...`);
    const dumpPath = path.join(tempDir, actualDbName);
    
    // Ki·ªÉm tra xem dumpPath c√≥ t·ªìn t·∫°i kh√¥ng
    try {
      await fs.access(dumpPath);
    } catch (error) {
      throw new Error(`Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c dump: ${dumpPath}`);
    }

    // N√©n th∆∞ m·ª•c dump th√†nh file .gz
    await compressDirectory(dumpPath, backupFilePath);

    // X√≥a th∆∞ m·ª•c temp
    await fs.rm(tempDir, { recursive: true, force: true });

    // L·∫•y k√≠ch th∆∞·ªõc file
    const stats = await fs.stat(backupFilePath);
    const fileSize = stats.size;

    console.log(`‚úÖ [Backup] T·∫°o backup th√†nh c√¥ng: ${backupFileName} (${(fileSize / 1024 / 1024).toFixed(2)} MB)`);

    return {
      filePath: backupFilePath,
      filename: backupFileName,
      fileSize
    };
  } catch (error) {
    console.error('‚ùå [Backup] L·ªói khi t·∫°o backup:', error);
    throw error;
  }
}

/**
 * ‚úÖ N√©n th∆∞ m·ª•c th√†nh file .gz
 * @param {string} sourceDir - Th∆∞ m·ª•c c·∫ßn n√©n
 * @param {string} outputFile - File output
 * @param {number} compressionLevel - M·ª©c n√©n t·ª´ 1-9 (1=fast, 9=best)
 */
async function compressDirectory(sourceDir, outputFile, compressionLevel = 6) {
  try {
    // ƒê·∫£m b·∫£o compressionLevel trong kho·∫£ng 1-9
    const level = Math.max(1, Math.min(9, compressionLevel || 6));
    
    // Th·ª≠ d√πng tar package tr∆∞·ªõc (h·ªó tr·ª£ compression level)
    const tar = require('tar');
    await tar.create(
      {
        gzip: {
          level: level // M·ª©c n√©n (1-9)
        },
        file: outputFile,
        cwd: path.dirname(sourceDir),
      },
      [path.basename(sourceDir)]
    );
    console.log(`‚úÖ [Backup] ƒê√£ n√©n v·ªõi compression level ${level}`);
  } catch (error) {
    // Fallback: d√πng tar command n·∫øu package tar kh√¥ng c√≥ ho·∫∑c l·ªói
    console.log('‚ö†Ô∏è [Backup] D√πng tar command thay v√¨ package tar');
    // Tar command kh√¥ng h·ªó tr·ª£ compression level tr·ª±c ti·∫øp, d√πng gzip ri√™ng
    const tarCmd = process.platform === 'win32' 
      ? `tar -czf "${outputFile}" -C "${path.dirname(sourceDir)}" "${path.basename(sourceDir)}"`
      : `tar -czf "${outputFile}" -C "${path.dirname(sourceDir)}" "${path.basename(sourceDir)}"`;
    await execAsync(tarCmd);
  }
}

/**
 * ‚úÖ Restore backup t·ª´ file .gz
 * @param {string} backupFilePath - ƒê∆∞·ªùng d·∫´n file backup
 * @param {string} dbName - T√™n database ƒë·ªÉ restore
 */
async function restoreMongoBackup(backupFilePath, dbName) {
  try {
    const mongoUri = process.env.MONGODB_URI || process.env.MONGO_URI || 'mongodb://localhost:27017/eduschool';
    
    // Parse database name
    let actualDbName = dbName;
    if (!actualDbName && mongoUri.includes('/')) {
      const uriParts = mongoUri.split('/');
      actualDbName = uriParts[uriParts.length - 1].split('?')[0];
    }
    if (!actualDbName) {
      actualDbName = 'eduschool';
    }

    // Gi·∫£i n√©n file backup
    const tempDir = path.join(path.dirname(backupFilePath), `restore-temp-${Date.now()}`);
    await fs.mkdir(tempDir, { recursive: true });

    console.log(`üîÑ [Restore] ƒêang gi·∫£i n√©n backup...`);
    
    // Gi·∫£i n√©n file .gz
    try {
      const tar = require('tar');
      await tar.extract({
        file: backupFilePath,
        cwd: tempDir,
      });
    } catch (error) {
      // Fallback: d√πng tar command
      console.log('‚ö†Ô∏è [Restore] D√πng tar command thay v√¨ package tar');
      const tarCmd = process.platform === 'win32'
        ? `tar -xzf "${backupFilePath}" -C "${tempDir}"`
        : `tar -xzf "${backupFilePath}" -C "${tempDir}"`;
      await execAsync(tarCmd);
    }

    // T√¨m th∆∞ m·ª•c dump
    const entries = await fs.readdir(tempDir);
    const dumpDir = entries.find(entry => {
      const fullPath = path.join(tempDir, entry);
      return fs.stat(fullPath).then(stat => stat.isDirectory()).catch(() => false);
    });

    if (!dumpDir) {
      throw new Error('Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c dump trong backup');
    }

    const dumpPath = path.join(tempDir, dumpDir);

    // Restore b·∫±ng mongorestore
    console.log(`üîÑ [Restore] ƒêang restore database...`);
    const mongorestoreCmd = `mongorestore --uri="${mongoUri}" --db="${actualDbName}" --drop "${dumpPath}"`;
    
    const { stdout, stderr } = await execAsync(mongorestoreCmd);
    
    if (stderr && !stderr.includes('finished') && !stderr.includes('done')) {
      console.warn(`‚ö†Ô∏è [Restore] mongorestore stderr: ${stderr}`);
    }

    // X√≥a th∆∞ m·ª•c temp
    await fs.rm(tempDir, { recursive: true, force: true });

    console.log(`‚úÖ [Restore] Restore backup th√†nh c√¥ng`);
  } catch (error) {
    console.error('‚ùå [Restore] L·ªói khi restore backup:', error);
    throw error;
  }
}

/**
 * ‚úÖ X√≥a c√°c backup c≈© (d·ª±a tr√™n retentionMonths)
 */
async function cleanupOldBackups(backupDir, retentionMonths) {
  try {
    const files = await fs.readdir(backupDir);
    const now = new Date();
    const cutoffDate = new Date(now.getFullYear(), now.getMonth() - retentionMonths, now.getDate());

    let deletedCount = 0;
    for (const file of files) {
      if (!file.startsWith('backup-') || !file.endsWith('.gz')) continue;

      const filePath = path.join(backupDir, file);
      const stats = await fs.stat(filePath);
      
      if (stats.mtime < cutoffDate) {
        await fs.unlink(filePath);
        deletedCount++;
        console.log(`üóëÔ∏è [Backup] ƒê√£ x√≥a backup c≈©: ${file}`);
      }
    }

    if (deletedCount > 0) {
      console.log(`‚úÖ [Backup] ƒê√£ x√≥a ${deletedCount} backup c≈©`);
    }

    return deletedCount;
  } catch (error) {
    console.error('‚ùå [Backup] L·ªói khi cleanup backup c≈©:', error);
    throw error;
  }
}

/**
 * ‚úÖ T·∫°o backup MongoDB b·∫±ng c√°ch export d·ªØ li·ªáu t·ª´ c√°c collection ra JSON
 * Kh√¥ng c·∫ßn mongodump, ch·ªâ c·∫ßn Node.js v√† Mongoose
 * @param {string} outputDir - Th∆∞ m·ª•c l∆∞u backup
 * @param {Object} options - T√πy ch·ªçn: 
 *   - collections: m·∫£ng t√™n collection c·∫ßn backup
 *   - excludeCollections: m·∫£ng t√™n collection c·∫ßn lo·∫°i b·ªè
 *   - excludeLargeCollections: boolean, t·ª± ƒë·ªông lo·∫°i b·ªè collections l·ªõn (logs, audit)
 *   - compressionLevel: s·ªë t·ª´ 1-9 (1=fast, 9=best compression)
 *   - onProgress: callback function ƒë·ªÉ c·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
 * @returns {Promise<{filePath: string, filename: string, fileSize: number, metadata: object}>}
 */
async function createMongoBackupJSON(outputDir, options = {}) {
  try {
    const { 
      collections = null, 
      excludeCollections = [],
      excludeLargeCollections = true, // M·∫∑c ƒë·ªãnh lo·∫°i b·ªè logs v√† audit
      compressionLevel = 6, // M·ª©c n√©n m·∫∑c ƒë·ªãnh (1-9)
      onProgress = null // Callback ƒë·ªÉ c·∫≠p nh·∫≠t progress
    } = options;
    
    // ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
    await fs.mkdir(outputDir, { recursive: true });

    // T·∫°o t√™n file backup v·ªõi timestamp
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
    const backupFileName = `backup-json-${timestamp}.tar.gz`;
    const backupFilePath = path.join(outputDir, backupFileName);

    // T·∫°o th∆∞ m·ª•c temp ƒë·ªÉ l∆∞u c√°c file JSON
    const tempDir = path.join(outputDir, `temp-json-${timestamp}`);
    await fs.mkdir(tempDir, { recursive: true });

    console.log(`üîÑ [Backup JSON] B·∫Øt ƒë·∫ßu export d·ªØ li·ªáu t·ª´ MongoDB...`);

    // L·∫•y connection t·ª´ mongoose
    const db = mongoose.connection.db;
    if (!db) {
      throw new Error('MongoDB connection ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o');
    }

    // L·∫•y danh s√°ch t·∫•t c·∫£ collections
    const allCollections = await db.listCollections().toArray();
    const collectionNames = allCollections.map(c => c.name);

    // L·ªçc collections theo options
    let collectionsToBackup = collectionNames;
    if (collections && Array.isArray(collections) && collections.length > 0) {
      collectionsToBackup = collections.filter(c => collectionNames.includes(c));
    }
    
    // ‚úÖ Danh s√°ch collections l·ªõn/th∆∞·ªùng kh√¥ng c·∫ßn backup (logs, audit)
    const largeCollectionsToExclude = [
      'auditlogs',
      'emaillogs', 
      'auditlog',
      'emaillog',
      'logs',
      'systemlogs'
    ];

    // Lo·∫°i b·ªè c√°c collections kh√¥ng c·∫ßn backup
    collectionsToBackup = collectionsToBackup.filter(c => {
      // Lo·∫°i b·ªè system collections
      if (c.startsWith('system.')) return false;
      // Lo·∫°i b·ªè collections trong exclude list
      if (excludeCollections.includes(c)) return false;
      // Lo·∫°i b·ªè collections l·ªõn n·∫øu excludeLargeCollections = true
      if (excludeLargeCollections && largeCollectionsToExclude.some(lc => 
        c.toLowerCase().includes(lc.toLowerCase())
      )) {
        return false;
      }
      return true;
    });

    console.log(`üìã [Backup JSON] S·∫Ω backup ${collectionsToBackup.length} collections: ${collectionsToBackup.join(', ')}`);
    
    // ‚úÖ G·ª≠i progress ban ƒë·∫ßu
    if (onProgress) {
      onProgress({
        stage: 'preparing',
        current: 0,
        total: collectionsToBackup.length,
        message: `Chu·∫©n b·ªã backup ${collectionsToBackup.length} collections...`,
        percentage: 0
      });
    }

    // Export t·ª´ng collection
    const backupMetadata = {
      timestamp: new Date().toISOString(),
      collections: [],
      totalDocuments: 0,
      totalSize: 0,
      excludedCollections: excludeLargeCollections ? largeCollectionsToExclude : []
    };

    let currentIndex = 0;
    for (const collectionName of collectionsToBackup) {
      currentIndex++;
      try {
        console.log(`üì¶ [Backup JSON] ƒêang export collection: ${collectionName}... (${currentIndex}/${collectionsToBackup.length})`);
        
        // ‚úÖ C·∫≠p nh·∫≠t progress
        if (onProgress) {
          onProgress({
            stage: 'exporting',
            current: currentIndex,
            total: collectionsToBackup.length,
            message: `ƒêang export collection: ${collectionName}`,
            percentage: Math.round((currentIndex / collectionsToBackup.length) * 60), // 60% cho export
            currentCollection: collectionName
          });
        }
        
        const collection = db.collection(collectionName);
        const documents = await collection.find({}).toArray();
        
        // Chuy·ªÉn ƒë·ªïi ObjectId v√† c√°c ki·ªÉu ƒë·∫∑c bi·ªát th√†nh JSON-safe
        const jsonData = documents.map(doc => {
          const json = JSON.parse(JSON.stringify(doc));
          return json;
        });

        // L∆∞u ra file JSON
        const jsonFileName = `${collectionName}.json`;
        const jsonFilePath = path.join(tempDir, jsonFileName);
        await fs.writeFile(jsonFilePath, JSON.stringify(jsonData, null, 2), 'utf8');

        const stats = await fs.stat(jsonFilePath);
        const fileSize = stats.size;

        backupMetadata.collections.push({
          name: collectionName,
          documentCount: documents.length,
          fileSize: fileSize
        });
        backupMetadata.totalDocuments += documents.length;
        backupMetadata.totalSize += fileSize;

        console.log(`‚úÖ [Backup JSON] ƒê√£ export ${collectionName}: ${documents.length} documents (${(fileSize / 1024).toFixed(2)} KB)`);
      } catch (error) {
        console.error(`‚ùå [Backup JSON] L·ªói khi export collection ${collectionName}:`, error.message);
        // Ti·∫øp t·ª•c v·ªõi collection ti·∫øp theo
      }
    }

    // L∆∞u metadata
    const metadataPath = path.join(tempDir, '_metadata.json');
    await fs.writeFile(metadataPath, JSON.stringify(backupMetadata, null, 2), 'utf8');

    // ‚úÖ C·∫≠p nh·∫≠t progress - b·∫Øt ƒë·∫ßu n√©n
    if (onProgress) {
      onProgress({
        stage: 'compressing',
        current: collectionsToBackup.length,
        total: collectionsToBackup.length,
        message: 'ƒêang n√©n c√°c file JSON...',
        percentage: 70
      });
    }

    // N√©n t·∫•t c·∫£ file JSON th√†nh file .tar.gz
    console.log(`üîÑ [Backup JSON] ƒêang n√©n c√°c file JSON v·ªõi compression level ${compressionLevel}...`);
    await compressDirectory(tempDir, backupFilePath, compressionLevel);

    // X√≥a th∆∞ m·ª•c temp
    await fs.rm(tempDir, { recursive: true, force: true });

    // L·∫•y k√≠ch th∆∞·ªõc file
    const stats = await fs.stat(backupFilePath);
    const fileSize = stats.size;

    // ‚úÖ C·∫≠p nh·∫≠t progress - ho√†n th√†nh
    if (onProgress) {
      onProgress({
        stage: 'completed',
        current: collectionsToBackup.length,
        total: collectionsToBackup.length,
        message: 'Backup ho√†n th√†nh!',
        percentage: 100
      });
    }

    console.log(`‚úÖ [Backup JSON] T·∫°o backup th√†nh c√¥ng: ${backupFileName}`);
    console.log(`   - T·ªïng s·ªë collections: ${backupMetadata.collections.length}`);
    console.log(`   - T·ªïng s·ªë documents: ${backupMetadata.totalDocuments}`);
    console.log(`   - K√≠ch th∆∞·ªõc file: ${(fileSize / 1024 / 1024).toFixed(2)} MB`);
    console.log(`   - Compression level: ${compressionLevel}`);
    if (excludeLargeCollections) {
      console.log(`   - ƒê√£ lo·∫°i b·ªè collections l·ªõn: ${largeCollectionsToExclude.join(', ')}`);
    }

    return {
      filePath: backupFilePath,
      filename: backupFileName,
      fileSize,
      metadata: backupMetadata
    };
  } catch (error) {
    console.error('‚ùå [Backup JSON] L·ªói khi t·∫°o backup:', error);
    throw error;
  }
}

/**
 * ‚úÖ Restore backup t·ª´ file JSON
 * @param {string} backupFilePath - ƒê∆∞·ªùng d·∫´n file backup
 * @param {Object} options - T√πy ch·ªçn: collections (m·∫£ng t√™n collection), dropExisting
 */
async function restoreMongoBackupJSON(backupFilePath, options = {}) {
  try {
    const { collections = null, dropExisting = false } = options;
    
    console.log(`üîÑ [Restore JSON] B·∫Øt ƒë·∫ßu restore t·ª´ file: ${backupFilePath}`);

    // T·∫°o th∆∞ m·ª•c temp ƒë·ªÉ gi·∫£i n√©n
    const tempDir = path.join(path.dirname(backupFilePath), `restore-json-temp-${Date.now()}`);
    await fs.mkdir(tempDir, { recursive: true });

    // Gi·∫£i n√©n file .tar.gz
    console.log(`üîÑ [Restore JSON] ƒêang gi·∫£i n√©n backup...`);
    try {
      const tar = require('tar');
      await tar.extract({
        file: backupFilePath,
        cwd: tempDir,
      });
    } catch (error) {
      // Fallback: d√πng tar command
      console.log('‚ö†Ô∏è [Restore JSON] D√πng tar command thay v√¨ package tar');
      const tarCmd = process.platform === 'win32'
        ? `tar -xzf "${backupFilePath}" -C "${tempDir}"`
        : `tar -xzf "${backupFilePath}" -C "${tempDir}"`;
      await execAsync(tarCmd);
    }

    // ƒê·ªçc metadata
    const metadataPath = path.join(tempDir, '_metadata.json');
    let metadata = null;
    try {
      const metadataContent = await fs.readFile(metadataPath, 'utf8');
      metadata = JSON.parse(metadataContent);
      console.log(`üìã [Restore JSON] Backup t·ª´: ${metadata.timestamp}`);
      console.log(`üìã [Restore JSON] S·ªë collections: ${metadata.collections.length}`);
    } catch (error) {
      console.warn('‚ö†Ô∏è [Restore JSON] Kh√¥ng t√¨m th·∫•y metadata, s·∫Ω restore t·∫•t c·∫£ file JSON');
    }

    // L·∫•y connection
    const db = mongoose.connection.db;
    if (!db) {
      throw new Error('MongoDB connection ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o');
    }

    // ƒê·ªçc t·∫•t c·∫£ file JSON trong th∆∞ m·ª•c temp
    const files = await fs.readdir(tempDir);
    const jsonFiles = files.filter(f => f.endsWith('.json') && f !== '_metadata.json');

    // L·ªçc collections theo options
    let collectionsToRestore = jsonFiles.map(f => f.replace('.json', ''));
    if (collections && Array.isArray(collections) && collections.length > 0) {
      collectionsToRestore = collections.filter(c => jsonFiles.includes(`${c}.json`));
    }

    console.log(`üì¶ [Restore JSON] S·∫Ω restore ${collectionsToRestore.length} collections`);

    // Restore t·ª´ng collection
    for (const collectionName of collectionsToRestore) {
      try {
        const jsonFilePath = path.join(tempDir, `${collectionName}.json`);
        const jsonContent = await fs.readFile(jsonFilePath, 'utf8');
        const documents = JSON.parse(jsonContent);

        if (!Array.isArray(documents) || documents.length === 0) {
          console.log(`‚ö†Ô∏è [Restore JSON] Collection ${collectionName} r·ªóng, b·ªè qua`);
          continue;
        }

        const collection = db.collection(collectionName);

        // X√≥a collection c≈© n·∫øu dropExisting = true
        if (dropExisting) {
          console.log(`üóëÔ∏è [Restore JSON] ƒêang x√≥a collection c≈©: ${collectionName}...`);
          await collection.drop().catch(() => {
            // Collection c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i, kh√¥ng sao
          });
        }

        // Insert documents
        console.log(`üì• [Restore JSON] ƒêang restore collection: ${collectionName} (${documents.length} documents)...`);
        
        // Chia nh·ªè th√†nh batch ƒë·ªÉ tr√°nh memory issue
        const batchSize = 1000;
        for (let i = 0; i < documents.length; i += batchSize) {
          const batch = documents.slice(i, i + batchSize);
          await collection.insertMany(batch, { ordered: false });
        }

        console.log(`‚úÖ [Restore JSON] ƒê√£ restore ${collectionName}: ${documents.length} documents`);
      } catch (error) {
        console.error(`‚ùå [Restore JSON] L·ªói khi restore collection ${collectionName}:`, error.message);
        // Ti·∫øp t·ª•c v·ªõi collection ti·∫øp theo
      }
    }

    // X√≥a th∆∞ m·ª•c temp
    await fs.rm(tempDir, { recursive: true, force: true });

    console.log(`‚úÖ [Restore JSON] Restore backup th√†nh c√¥ng`);
  } catch (error) {
    console.error('‚ùå [Restore JSON] L·ªói khi restore backup:', error);
    throw error;
  }
}

module.exports = {
  createMongoBackup,
  createMongoBackupJSON,
  restoreMongoBackup,
  restoreMongoBackupJSON,
  cleanupOldBackups
};

